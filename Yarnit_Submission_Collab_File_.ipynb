{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install backoff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSCL9yzYsnG0",
        "outputId": "ab3b00c0-2105-49bc-bfd8-76eb17020e3b"
      },
      "id": "rSCL9yzYsnG0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting backoff\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: backoff\n",
            "Successfully installed backoff-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXxwOOEZsu1l",
        "outputId": "e74c0986-5061-4ee8-b0a3-52a2526449fc"
      },
      "id": "xXxwOOEZsu1l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.26.0-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "712d3597",
      "metadata": {
        "id": "712d3597"
      },
      "source": [
        "## for gpt 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f56d5c",
      "metadata": {
        "id": "e1f56d5c"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "from typing import Any, Callable\n",
        "import backoff\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message=\".*deprecated.*\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client= OpenAI(\n",
        "    api_key = ''\n",
        ")\n",
        "#gpt-3.5-turbo\n"
      ],
      "metadata": {
        "id": "eu9WakZ6xGNH"
      },
      "id": "eu9WakZ6xGNH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3398bf2b",
      "metadata": {
        "id": "3398bf2b"
      },
      "outputs": [],
      "source": [
        "def ai_predict(final_text: str, model='gpt-3.5-turbo') -> str:\n",
        "    \"\"\"This function uses any ai model to provide an answer based on the text\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            temperature =0.4,\n",
        "            messages=[{\"role\": \"user\", \"content\": final_text}],\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        raise (e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic = \"Generative A.I\"\n",
        "format = \"LinkedIn Post\""
      ],
      "metadata": {
        "id": "R9T7FkIjm2yh"
      },
      "id": "R9T7FkIjm2yh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"For the given format like {format} which is based on this {topic} as the expert in this {topic} for this {format} generate the Content for the same\""
      ],
      "metadata": {
        "id": "aQgNCJqdkJSP"
      },
      "id": "aQgNCJqdkJSP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_predict(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dOOOettnFZc",
        "outputId": "5f59662f-75c3-4350-c022-f5a8e90bf7af"
      },
      "id": "0dOOOettnFZc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exciting news in the world of Generative A.I! Our team of experts has been hard at work developing cutting-edge technology that is revolutionizing the way we interact with artificial intelligence. From creating realistic human-like conversations to generating creative content, our Generative A.I is pushing the boundaries of what is possible.\n",
            "\n",
            "With our state-of-the-art algorithms and machine learning techniques, we are paving the way for a future where A.I can assist us in ways we never thought possible. Join us on this journey as we continue to innovate and shape the future of technology.\n",
            "\n",
            "#GenerativeAI #ArtificialIntelligence #Innovation #Technology #FutureTech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import traceback\n",
        "from typing import Any, Callable\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "import warnings\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "TAT8RN49vvkp"
      },
      "id": "TAT8RN49vvkp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            paragraphs = soup.find_all('p')\n",
        "            text = '\\n'.join([p.get_text() for p in paragraphs])\n",
        "            return text\n",
        "        else:\n",
        "            print(\"Failed to retrieve content. Status code:\", response.status_code)\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "        return None"
      ],
      "metadata": {
        "id": "k1GGYE6JvPvZ"
      },
      "id": "k1GGYE6JvPvZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify"
      ],
      "metadata": {
        "id": "9a2jkeOTv7Sk"
      },
      "id": "9a2jkeOTv7Sk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Generative_artificial_intelligence\"\n",
        "question = \"What are the concerns around Generative AI?\""
      ],
      "metadata": {
        "id": "r1czo-ZLww2B"
      },
      "id": "r1czo-ZLww2B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_from_url(url,question):\n",
        "    text = retrieve_text_from_url(url)\n",
        "    response = \"I don't know the answer.\"\n",
        "\n",
        "    prompt = f\"You are provided with this text: {text}. As a good reader, read the text properly. Now, answer the following question: {question}. Your answer should be based solely on the content you have read.Don't Include Other Innessary things Other than Answer in Responce and If you don't found the Answer then Return this {response} as Final Answer\"\n",
        "    generated_content = ai_predict(prompt)\n",
        "\n",
        "    return jsonify({'text': generated_content})\n"
      ],
      "metadata": {
        "id": "J4D6zJH0vf1j"
      },
      "id": "J4D6zJH0vf1j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_jK-d7mxDSP"
      },
      "id": "g_jK-d7mxDSP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}